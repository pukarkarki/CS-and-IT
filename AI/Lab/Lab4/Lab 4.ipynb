{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8da56423-bcf1-49f7-9bf3-8f3070d45ae0",
   "metadata": {},
   "source": [
    "<h2> <b>Lab 4: Supervised Learning - Naive Bayesian CLassifier </b><h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9d6bef-367e-45f5-ac30-7d26ff228e92",
   "metadata": {},
   "source": [
    "<h4 align=\"justify\"> Classification in machine learning and statistics is a supervised learning approach in which the computer program learns from the data given to it and make new observations or classifications. Classification is a process of categorizing a given set of data into classes. The most common classification problems are – speech recognition, face detection, handwriting recognition, document classification, etc. It can be either a binary classification problem or a multi-class problem too. There are a bunch of machine learning algorithms for classification in machine learning. \n",
    "In this lab we will be looking at Naive Bayesian Classifier.</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c96fc4-198e-4e63-ac65-0cd0308514e4",
   "metadata": {},
   "source": [
    "<h3> We will use a library called scikit-learn to build our model </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16250ced-5198-49b4-b3d9-85dc8908367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the numpy and scikit-learn library as follows\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590abcc5-18c6-4571-98a4-eed181c49144",
   "metadata": {},
   "source": [
    "<h4> We will be using built-in datasets of sklearn and sklearn comes with the following datasets </h4>\n",
    "<ul>\n",
    "<li>load_boston - Load and return the boston house-prices dataset (regression).\n",
    "\n",
    "<li>load_iris - Load and return the iris dataset (classification).\n",
    "\n",
    "<li>load_diabetes - Load and return the diabetes dataset (regression).\n",
    "\n",
    "<li>load_digits - Load and return the digits dataset (classification).\n",
    "\n",
    "<li>load_linnerud - Load and return the physical excercise linnerud dataset.\n",
    "\n",
    "<li>load_wine - Load and return the wine dataset (classification).\n",
    "\n",
    "<li>load_breast_cancer - Load and return the breast cancer wisconsin dataset (classification).\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0b1dad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we begin by loading the iris dataset\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "283f666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddca8619-2e7f-4606-8a47-aa115142101a",
   "metadata": {},
   "source": [
    "<h4> The loaded data is in python dictionary format with the following keys.\n",
    "<ul>\n",
    "   <li> target_names\n",
    "   <li> target\n",
    "   <li> feature_names\n",
    "   <li> data\n",
    "</ul>\n",
    "And, we will load the data into different variables as follows. </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08c6af08",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = data['target_names']\n",
    "labels = data['target']\n",
    "feature_names = data['feature_names']\n",
    "features = data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31c1ff02-88b2-4707-b7d4-113b822be5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "print(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a835d7e-c3c2-4334-b7dd-e8197c7e3387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbb781a8-28a0-405f-8418-1574577c2f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a13c519-60fa-434a-9701-87006ef2161b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750c6a70-064a-4dbd-a9c0-cf576d6f7334",
   "metadata": {},
   "source": [
    "<h4 aligh='justify'> Now we will split the data into training set and test set. We train our classifier using training set data and we use the test set\n",
    "to see how well our model performs on the unseen data. </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33c10f41-5c33-4422-b4a0-199eaa595825",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42c77094-5b0a-4af7-913a-51284599f19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features,labels,test_size = 0.330, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2db934c-50fa-4e4f-91e1-d56a022828c5",
   "metadata": {},
   "source": [
    "<h4> We will be building our model. We are going to use Naïve Bayes algorithm for building the model.</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc917110-0471-4172-adc0-cef104a7b6c0",
   "metadata": {},
   "source": [
    "<h4 align='justify'>In the case of categorical variables, such as counts or labels, a multinomial distribution can be used.\n",
    "If the variables are binary, such as yes/no or true/false, a binomial distribution can be used. If a variable is numerical,\n",
    "such as a measurement, often a Gaussian distribution is used.\n",
    "<ul>\n",
    "<li>Binary: Binomial distribution.\n",
    "<li>Categorical: Multinomial distribution.\n",
    "<li>Numeric: Gaussian distribution.\n",
    "</ul>\n",
    "These three distributions are so common that the Naive Bayes implementation is often named after the distribution.For example:\n",
    "<ul>\n",
    "<li>Binomial Naive Bayes: Naive Bayes that uses a binomial distribution.\n",
    "<li>Multinomial Naive Bayes: Naive Bayes that uses a multinomial distribution.\n",
    "<li>Gaussian Naive Bayes: Naive Bayes that uses a Gaussian distribution.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f80cb9ea-d8aa-411e-a7ea-229a464e864e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "238f50a5-e3a3-472b-a251-6252e9d2a013",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a9bd542-4cd6-42bc-bd53-6d4a8e5c9001",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab0e31e-eaa5-4d98-8dc6-af32495a9bcf",
   "metadata": {},
   "source": [
    "<h4> We are going to evaluate the model by making predictions on our test data. For making predictions, we will use the predict() function.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddbccf27-f24b-4c73-8f50-54457b82de81",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da888705-403a-49ae-97f6-d43ae6877a74",
   "metadata": {},
   "source": [
    "<h4 align = 'justify'> Now we check the prediction of our model with the actual output. Then we count the number of correctly classified\n",
    "instances and calculate the accuracy by dividing it by the size of our test data.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e9f9693-7c7e-4878-b5e2-fe24dfe2e00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is  96.0\n"
     ]
    }
   ],
   "source": [
    "result = y_test == preds\n",
    "correct_preds = result.sum()\n",
    "accuracy = correct_preds/y_test.size * 100\n",
    "print(\"The accuracy is \",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f316738-da02-40ee-b0e4-d2518d1a34e2",
   "metadata": {},
   "source": [
    "<h4>We can also directly calculate the accuracy by using accuracy_score() <b>from sklearn.metrics import accuracy_score</b></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3606589-f03b-437c-821a-0a0f3b186799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is  96.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"The accuracy is \",accuracy_score(y_test, preds)*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
